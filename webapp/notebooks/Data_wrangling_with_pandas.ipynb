{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Hackathon Bootcamp - Data wrangling workflow\n", "\n", "First we'll import some libraries, if you get a `ModuleNotFoundError`, you can simply install it from the Terminal in MacOS and Linux or the Anaconda prompt in windows, using the command: `conda install <module>` or if that fails `pip install <module>`.\n", "\n", "So what is **Pandas**? \n", "It stands for _\"Panelled Data Sets\u201d_, so think spreadsheets, BIG spreadsheets.\n", "\n", "It's worth noting before we jump in that these amazing libraries (Pandas, NumPy, SciPy, etc) are written by folk who are alive today and who have been amazingly generous with their time. Check out [Wes McKinney's Twitter account](https://twitter.com/wesmckinn) for example, the creator of Pandas. Pandas has got over 10Mio downloads (near 12Mio??).\n", "\n", "If you've heard of TensorFlow, it's a similar story: Google opened it up and doubled the number of developers working on it to make it better.\n", "\n", "The main point here is that FOSS is simply amazing, and you're all on the brink of contributing to it yourselves: you're in good company!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Be sure to use the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/index.html) if you get stuck. Another great resource that [stackoverflow](https://stackoverflow.com) often points me to is [Chris Albon's website](https://chrisalbon.com/)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now to load some data, we'll use pandas `read_csv()` function, you can use the help methods you learned in `Bootcamp_beginner_track` to see how to use it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../data/training_DataFrame_processed.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our data loaded, let's start by exploring it."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Explore the dataset with pandas functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Cleaning Dataframe, dropping and creating columns\n", "\n", "In `df.head()` and `df.nunique()` we saw that there is an 'Unnamed: 0' column that we can use as a UID, we will set the index of the Dataframe using this column instead of the automatically generated column, we'll then assign the name of the index to 'UID'. Note that we can also change the name of the colum first and then set the index.\n", "\n", "There are no `NaN` values in this dataset, as can be seen from `df.info()` and `df.describe()`, so we will not need to `dropna()`.\n", "\n", "Likewise we can check and see there are no duplicates and so we will not need `drop_duplicates()` either on this dataset.\n", "\n", "`dropna()` and `drop_duplicates()` are two functions that you may use often, but you must always chose carefuly _what_ you do with `NaN` values and duplicated rows."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We are now going to drop the 'NM_M' column as we will not use it further, and create a new column called 'Depth_ft' for use by non-SI contractors for example."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Investigate data visually\n", "\n", "We will first import [seaborn](https://seaborn.pydata.org/), a statistical data visualization package in Python that will allow us to very rapidly look into our data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import seaborn as sns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.columns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The Dataframe we used above is actually already pretty clean, as we can see from `sns.pairplot()`. So now in order to explore Pandas a little more, we will load up a new data set that has not been cleaned yet. The data are interpreted image data showing different data in an inclined borehole.\n", "\n", "#### Load and explore the data\n", "\n", "We'll start by loading the data, and will then manipulate the resulting Dataframe to illustrate some of Pandas functions."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_img = pd.read_csv('../data/image_data.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Check for any duplicates as the UID column must be... unique."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that in this data set, it's not uncommon to see duplicate Depths, which we can see using the same method as to check the 'Unnamed: 0' column:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_img[df_img.duplicated(subset='#DEPTH') == True]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll come back to slicing and indexing, but this should look familiar and allows us to see the duplicated depths easily."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_img.iloc[350:352]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now rename 'Unnamed: 0' to 'UID' and then use that as the index column, as we did before."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Check that all changes have been carried out successfully:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_img.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Slicing and indexing into Dataframe\n", "\n", "We can index and slice into Dataframes in many ways, for example:\n", "\n", "- slicing, indexing and stepping into columns as we did for lists\n", "- `df.iloc[]` for \"Purely integer-location based indexing for selection by position.\" (see [docs](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html))\n", "- `df.loc[]` to \"Access a group of rows and columns by label(s) or a boolean array.\" (see [docs](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html))\n", "\n", "Be sure to see the [docs](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more details."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_img.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Remove bad or missing data\n", "\n", "In this data set there are no 'bad or missing' data _per se_ but there _are_ some data points which we won't look at, so let's start by learning how to drop those rows (you've already seen `pd.drop()` for columns above when we dropped 'NM_M' from the `training_DataFrame_processed.csv` Dataframe.\n", "\n", "Here we'll drop both 'BED' and 'BED_LOW_CONF' from the `df_img` Dataframe."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now as always, let's check we made the correct changes:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_img.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["But as you see, we need to correct the 'UID' column _if_ we want it to run sequentially. Note that in Pandas, the index column _does not need_ to be a sequence starting at 0, they don't even need to be unique in fact.\n", "\n", "It is good practise as we've been doing so far to keep the index label unique, such as in a time series, if nothing else it makes for easier reading. There is also a [performance hit](https://stackoverflow.com/questions/16626058/what-is-the-performance-impact-of-non-unique-indexes-in-pandas) when using non-unique indices.\n", "\n", "This it will also make indexing easier as when we use `pd.iloc[]` the indices will match.\n", "\n", "So let's now reset the index from 0."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_img.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Add columns"]}, {"cell_type": "markdown", "metadata": {"tags": ["exe"]}, "source": ["<div class=\"alert alert-success\">\n", "<h3>Exercise</h3>\n", "\n", "We saw how to add a column above, try to add a column called '#DEPTH_ft'.", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Conditionals\n", "\n", "We often want to add new column values based on conditions, we can select rows based on a conditional like we did when we were looking at `duplicated` rows above, following this pattern:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's select all 'FRACTURE' rows:\n", "df_img[df_img['DIPTYPE'] == 'FRACTURE'].head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Conditionals with numpy\n", "\n", "We can also use another library - which we will not explore today - to acess and modify rows based on a condition, with `numpy.where()` which you can read about [here](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.where.html)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["And with `numpy` we can use `np.where()` too:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"tags": ["exe"]}, "source": ["<div class=\"alert alert-success\">\n", "<h3>Exercise</h3>\n", "\n", "With help from the cells above, try to add a new `df_img['ORIGIN']` column with values as follows, using `np.where()`:\n", "- `NATURAL` for natural fractures and,\n", "- `DRILLING` for others\n", "\n", "You can find out how many classes of fractures exist by using `df_img.DIPTYPE.unique()`.\n", "\n", "N.B.: build this up one step at a time, and remember that multiple conditions will need parentheses `()` surrounding each condition. You will need the boolean operator `|` if you want to represent an `or` case and the `&` if you want to represent an `and` case.", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Groupby\n", "\n", "Now that we have a new column with natural fractures as well as drilling related damage, we might want to group our data to see the relative amount of both kinds of damage, for this we can use `df.groupby()` and `.count()` that we have seen several times above, you can pass multiple columns to `.groupby()`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Reshaping Dataframes\n", "\n", "Dealing with tabular data, we often want to rotate, flatten or pivot data tables, Pandas allow us to do all this. \n", "\n", "In this tutorial, we will only look at:\n", "\n", "- `df.pivot()`\n", "- `pd.melt()`\n", "\n", "But you can find full details if you refer to the [documentation](https://pandas.pydata.org/pandas-docs/stable/reshaping.html) - as always:\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Pivot: `df.pivot()`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Melt: `pd.melt()`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Handling multiple Dataframes\n", "\n", "Joining, merging, concatenating tabular data is a large topic, by now you know: I always refer to the [docs](https://pandas.pydata.org/pandas-docs/stable/merging.html) and often check out [Chris Albon's page](https://chrisalbon.com/python/data_wrangling/pandas_join_merge_dataframe/).\n", "\n", "In order to practise, let's load a new data set, we have four data frames representing:\n", "\n", "1. GR upper hole section\n", "2. GR lower hole section\n", "3. Resistivity upper hole section\n", "4. Resistivity lower hole section\n", "\n", "We want to join these so we end up with a single DataFrame containing all data.\n", "\n", "First import all four files:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_gr_upper = pd.read_csv('../data/df_gr_upper.csv', header=None, names=['GR'])\n", "df_res_upper = pd.read_csv('../data/df_res_upper.csv', header=None, names=['RES'])\n", "df_gr_lower = pd.read_csv('../data/df_gr_lower.csv', header=None, names=['GR'])\n", "df_res_lower = pd.read_csv('../data/df_res_lower.csv', header=None, names=['RES'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Making the GR and Res curves with `pd.concat()`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Making a full log with GR and Res using `df.join()`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Saving a modified DataFrame\n", "\n", "There are many output formats you can get including:\n", "\n", "- `df.to_csv()`\n", "- `df.to_excel()`\n", "- `df.to_html()`\n", "- `df.to_json()`\n", "- `df.to_feather()`\n", "- `df.to_pickle()`\n", "\n", "... and more"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<hr />\n", "\n", "<div>\n", "<img src=\"https://avatars1.githubusercontent.com/u/1692321?s=50\"><p style=\"text-align:center\">\u00a9 Agile Geoscience 2018</p>\n", "</div>"]}], "metadata": {"kernelspec": {"display_name": "geocomp", "language": "python", "name": "geocomp"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat": 4, "nbformat_minor": 2}